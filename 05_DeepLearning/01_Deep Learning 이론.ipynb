{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc831db-daaa-4ddd-b0ec-7038336a1529",
   "metadata": {},
   "source": [
    "# DeepLearning\n",
    "<br>\n",
    "\n",
    "## 기본 개념\n",
    "- 데이터로부터 특징 / 패턴을 학습\n",
    "\n",
    "## 딥러닝 목표\n",
    "- 모델에 입력값을 넣었을 때의 **출력값이 최대한 정답과 일치**하게 하는 것\n",
    "\n",
    "## 딥러닝 학습 방법\n",
    "- 딥러닝 모델의 매개변수(weight, bias)를 무작위로 부여한 후, **반복학습**을 통해 모델의 출력값을 정답과 일치하도록 **매개변수를 조금씩 조정**\n",
    "- ***Gradient Descent 최적화 알고리즘***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f569553-f645-4850-b19b-7841e5e73dd4",
   "metadata": {},
   "source": [
    "## 딥러닝 기술 원리\n",
    "### Perceptron 퍼셉트론 모델\n",
    "- 사람 두뇌에 있는 **뉴런을 모델링**하는 것 -> 간단한 함수를 학습할 수 있음\n",
    "### DNN (Deep Neural Network)\n",
    "- 입력층과 출력층 사이에 여러 개의 **은닉층(hidden layer)으로** 이루어진 인공신경망\n",
    "- 신경망 출력에 **비선형 활성화** 함수를 추가하여 복잡한 비선형 관계를 모델링할 수 있음\n",
    "- input layer -> (hidden layer1 -> hidden layer2 -> ... ) -> output layer (각 레이어에는 수많은 퍼셉트론이 존재한다)\n",
    "### Activation function\n",
    "- Binary Step\n",
    "- Logistic, sigmoid, or soft step\n",
    "- Hyperbolic tangent(tanh)\n",
    "- Rectified linear unit(ReLU)\n",
    "### Loss Function\n",
    "- 신경망 학습의 목적함수로 **출력값(예측값)과 정답(실제값)의 차이**를 계산 (차이를 작게 만드는 것이 목표)\n",
    "- 회귀 (Regression) : MSE, MAE\n",
    "- 분류 (Classification)\n",
    "    - 이진분류 (Binary cross-entropy)\n",
    "    - 다중분류 (Categorical cross-entropy)\n",
    "### Gradient Descent (경사 하강법)\n",
    "- 뉴럴넷이 가중치 파라미터들을 **최적화**하는 방법\n",
    "- 손실함수의 현 가중치에서 기울기(Gradient)를 구해서 **Loss를 줄이는 방향**으로 업데이트 해 나감\n",
    "### Forward Propagation (순전파)\n",
    "- 딥러닝 모델에 값을 입력해서 출력을 얻는 과정\n",
    "### Back Propagetion (역전파) (Error Backpropagetion)\n",
    "- 실제값과 모델 결과값에서 오차(error cost)를 구해서 오차를 output에서 input 방향으로 보냄\n",
    "- **가중치를 재업데이트** 하면서 학습\n",
    "### Optimization Algorithm\n",
    "- 딥러닝 모델의 매개변수(weight, bias)를 조절해서 **손실함수의 값을 최저로 만드는 과정** ex) 경사하강법(GD)\n",
    "- Adam : RMSProp + Mamentum 방향도 스텝 사이즈도 적절한 알고리즘\n",
    "### Data Set\n",
    "- 1 Epoch : 모든 데이터 셋을 한 번 학습 (너무 크기 때문에 batch size로 나누게 됨)\n",
    "- 1 iteration : 1회 학습\n",
    "- Minibatch : 데이터 셋을 batch size 크기로 쪼개서 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bb018a-dea0-4fe6-ad15-698641bc8d72",
   "metadata": {},
   "source": [
    "# 딥러닝 주요 알고리즘\n",
    "## DNN(Deep Neural Network 심층신경망)\n",
    "- Dropout : 과적합(overfitting) 방지용, Train 학습 시에만 사용\n",
    "- 라이브러리 임포트\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "```\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa288b1-fc89-45e4-9f65-9474c67151c1",
   "metadata": {},
   "source": [
    "#### 모델 생성 및 트레이닝\n",
    "~~~python\n",
    "# 모델 레이아웃 지정\n",
    "model=Sequential()\n",
    "model.add(Dense(4,input_shape=(3,),activation='relu'))  hidden layer 1\n",
    "model.add(Dropout(0.2))  # 안넣어도 되긴 함\n",
    "model.add(Dense(4,activation='relu')) # hidden layer 2\n",
    "model.add(Dense(1,activation='sigmoid')) # output layer\n",
    "\n",
    "\n",
    "# 컴파일. loss function(이진분류로 선택), 최적화 알고리즘(아담으로 지정), 성능지표(로 정확도 acc를 보겠다) 지정\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=40,batch_size=10)\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6199e3-ca0f-4456-9b34-2a06fc594c34",
   "metadata": {},
   "source": [
    "## CNN(Convolution Neural Network)\n",
    "- 이미지 분류에 주로 사용하게 됨\n",
    "- 이미지의 **특징을 추출(Feature extraction)** 하는 부분과 **클래스를 분류(Classification)** 하는 부분으로 나눔\n",
    "    - **특징 추출** 영역 : Convolution Layer와 Polling Layer를 여러 겹 쌓는 형태로 구성\n",
    "    - **클래스 분류** 영역 : Fully Connected Layer 구성하여 사용\n",
    "- DNN에서 이미지를 사용하지 않는 이유\n",
    "    - 이미지를 1차원 형태로 처리 : 위치정보 소실 -> **정보소실**\n",
    "    - 이미지 사이즈가 커질 경우 비례해서 학습할 가중치 증가\n",
    "- Convolutional Filter\n",
    "    - **가중치 필터**로 학습되며, 이미지 특징/패턴을 찾아내 이미지 분류가 가능함\n",
    "    - Feature Map을 만들게 된다\n",
    "- Padding\n",
    "    - Convolutional layer의 **출력 데이터가 줄어드는 것을 방지하는 방법**\n",
    "    - 입력 데이터의 외곽에 0으로 채워 넣는 것\n",
    "- Pooling Layer\n",
    "    - 계산량, 메모리 사용량, 파라미터 수를 줄이기 위해 **입력 이미지의 sub sample(축소본)을 만드는 것**\n",
    "    - 출력 데이터의 크기를 줄이거나 특정 데이터를 강조하는 용도로 사용\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea83f7-814d-4fd9-ad30-574a124a8825",
   "metadata": {},
   "source": [
    "#### CNN 구현\n",
    "~~~python\n",
    "# 모델 레이아웃 지정\n",
    "model=Sequential()\n",
    "model.add(Conv2D(12,kernel_size(5,5),activation='relu', input_shape=(120,60,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(12,kernel_size(5,5),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(12,kernel_size(5,5),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(4,actovatopn='softmax'))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f602b8-fbd2-49e2-bbc5-5fe7f78729b4",
   "metadata": {},
   "source": [
    "## RNN (Recurrent Neural Network 순환신경망)\n",
    "- 입력과 출력을 **시퀀스(sequence) 단위로 처리하는 모델**\n",
    "- **음성 인식, 언어 모델링, 번역, 이미지 주석 생성**에 활용\n",
    "- 히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는(directed cycle) 인공신경망의 한 종류\n",
    "- RNN 문제점 (vanishing gradient problem)\n",
    "    - RNN은 관련 정보와 그 정보를 사용하는 지점 사이 거리가 멀 경우 **역전파시 그래디언트가 점차 줄어 학습능력이 크게 저하**되는 것으로 알려져 있음\n",
    "    - 극복하기 위해 LSTM (Log short-Term Memory)가 사용됨 (RNN의 히든 state에 cell-state를 추가한 구조)\n",
    "<br>\n",
    "\n",
    "## LSTM\n",
    "- 오랫동안 정보를 전달할 수 있음\n",
    "- 문장과 같은 단어가 문장 안에서의 **순서가 중요한 경우**나 주가와 같은 **시계열 데이터**에 효과적\n",
    "- 구현 방법\n",
    "    - many to one : `tf.keras.layers.LSTM(64)`\n",
    "    - many to many : `tf.keras.layers.LSTM(64,return_sequences=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a78812-9647-4b45-97a9-588ad7af38b3",
   "metadata": {},
   "source": [
    "#### LSTM 구현\n",
    "```python\n",
    "model.add(LSTM(4,input_shape=(7,2),return_sequences=True)) -> 7 4\n",
    "model.add(LSTM(3,return_sequence=True)) -> #,7,3\n",
    "model.add(Flatten())\n",
    "model.add(dense(1,activation='sigmoid'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62b018-488d-4e9c-a69e-4c72c8d5431b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
