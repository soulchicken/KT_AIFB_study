{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5abfa6-1e41-4c1c-90c0-db15406221f2",
   "metadata": {},
   "source": [
    "# 머신러닝 프로세스\n",
    "데이터 수집 -> 데이터 정리 -> 테스트 데이터셋 & 모델 학습 -> 모델 테스트 -> 모델 배포\n",
    "\n",
    "# 머신러닝 기본 개념\n",
    "- 데이터와 결과를 기반으로 **스스로 패턴 학습**을 하고 이를 이용해서 예측\n",
    "\n",
    "## Linear Regression\n",
    "- (예시) 가설 : 공부를 많이 하면 공부를 잘 할 것이다\n",
    "    - 학습시간과 토익성적 데이터를 가져옴\n",
    "    - y = ax + b 꼴로 만들어지는 함수, 모델, 알고리즘을 생성 -> y = Wx (W는 가중치가 됨)\n",
    "- 컴퓨터는 W 가중치가 최적인지 어떻게 알 수 있을까?\n",
    "    - Cost Function 사용 : (실제값 - 예측값)2/N -> MSE(Mean Squared Error)\n",
    "- Gradient Descent Algorithm 경사하강법 (Cost Function 을 어떻게 최적화 할 것인가?)\n",
    "    - 그래프상에서 __비용이 제일 낮은 아래부분(기울기 0)__ 을 찾으면 된다\n",
    "    - cost가 가장 낮아지는 수준까지 진행하면서 가중치 업데이트 진행\n",
    "\n",
    "## 모델 학습이란?\n",
    "    - 목표 : 최적 직선 구하기\n",
    "    - 직선별 손실함수 구하기\n",
    "    - 손실 함수 최소값 구하기\n",
    "    - Gradient Descent Algorithm 이용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25835f09-6247-4a11-8e3c-4513fe5520fc",
   "metadata": {},
   "source": [
    "# 머신러닝 기술 원리\n",
    "<br>\n",
    "## 데이터 확보\n",
    "    - 잘 정리된 데이터 확보가 중요\n",
    "\n",
    "## 지도학습(Supervised Learning) VS 비지도학습(Un-Supervised Learning)\n",
    "- 지도학습\n",
    "    - 정답을 알려주면서 진행하는 학습\n",
    "    - 데이터와 레이블(정답)이 함께 제공됨\n",
    "    - 레이블(Lable) : 정답, 실제값, 타깃, 클래스, y\n",
    "    - 예측된 값 : 예측값, 분류값, y hat\n",
    "- 비지도학습\n",
    "    - 레이블이 없이 진행되는 학습\n",
    "    - 데이터 자체에서 패턴을 찾아내야 할 때 사용\n",
    "## 지도학습 모델 종류\n",
    "- 분류 모델 (Classification)\n",
    "    - 레이블의 값들이 **이산적으로 나눠질 수 있는** 문제에 사용\n",
    "- 예측 모델 (Regression, 회귀모델)\n",
    "    - 레이블의 값들이 **연속적인** 문제에 사용\n",
    "    \n",
    "## 지도학습 데이터셋 구조\n",
    "- 특징/속성(feature) : 각 열(column)\n",
    "    - 데이터 열(column) 중 하나를 선택해서 레이블로 사용함\n",
    "- 예제 (Example) 데이터 : 각 행(row)\n",
    "## 데이터셋 분리\n",
    "- DataSet : Train + (Validation) + Test\n",
    "\n",
    "## 모델 선택\n",
    "- 완만한 형태를 만드는 좋은 모델(Model)\n",
    "- 과한 형태를 만드는 과적합(Overfitting)은 맞지 않다\n",
    "- 학습을 덜해서 둔한 모델은 과소적합(Underfitting)도 맞지 않다\n",
    "\n",
    "## 모델 성능 평가\n",
    "- training set과 test set의 차이(error) 적어야한다\n",
    "\n",
    "## Confusion Matrix\n",
    "- 모델의 대략적인 성능확인과 모델의 성능을 오차행렬을 기반으로 수치로 표현\n",
    "- true Positive(TP) : 참을 참으로 예측 (정답)\n",
    "- True Negative(TN) : 거짓을 거짓으로 예측 (정답)\n",
    "- False Positive(FP) : 거짓을 참으로 예측 (오답)\n",
    "- False Negative(FN) : 참을 거짓으로 예측 (오답)\n",
    "\n",
    "## 성능지표\n",
    "- 학습이 끝난 후 **모델을 평가**하는 용도로 사용됨\n",
    "- 정밀도(Precision) : 모델이**True라고 분류**한 것 중에서 **실제 True**인 것의 비율\n",
    "    - 날씨 예측 모델이 맑다고 예측했을 때, 실제 날씨가 맑았는지 나타낸 지표\n",
    "    - Precision = TP / (TP + FP)\n",
    "- 재현율(Recall) : **실제 True**인 것 중에서 모델이 **True라고 예측**한 것의 비율\n",
    "    - 실제 날씨가 맑은 날 중에서 모델이 맑다고 예측한 비율을 나타낸 지표\n",
    "    - Recall = TP / (TP + FN)\n",
    "    \n",
    "\n",
    "- 정확도(Accuracy) : 가장 직관적으로 모델의 성능을 나타낼 수 있는 평가 지표\n",
    "    - 한 달에 맑은 날이 28일, 비오는 날이 2일인 경우, 비가 오는 것을 예측하는 성능은 매우 낮을 수 밖에 없다. 보완할 지표가 필요해진다\n",
    "    - Accuracy = (TP + TN) / (TP + FN + FP + TN)\n",
    "- F1 점수(F1-score) : 정밀도와 재현율의 조화평균\n",
    "    - (F1-score) = 2 * (1 / 1/Precision + 1/Recall) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894aefac-6d05-471c-8634-c75d61473d1a",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# 머신러닝 주요 알고리즘\n",
    "## Scikit-learn\n",
    "- 가장 인기 있는 머신러닝 패키지, **많은 머신러닝 알고리즘이 내장**되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2398e-2fe5-4411-815e-37d5489eb4a4",
   "metadata": {},
   "source": [
    "# 아래 방식으로 불러와서 사용한다\n",
    "~~~ python\n",
    "from sklean.family import Model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "print(model)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82daa1a-cf74-4b54-b962-b9c6b664c33d",
   "metadata": {},
   "source": [
    "# 머신러닝 주요 알고리즘 분류\n",
    "- 회귀 : Linear Regression\n",
    "- 분류 : Logistic Regression\n",
    "- 회귀분류(모두 가능) : Decision Tree, Random Forest, K-Nearest Neighbor\n",
    "<br>\n",
    "\n",
    "## Liear Regression\n",
    "- 패턴에 맞는 직선을 구해서 최적의 기울기를 구하는 방법\n",
    "- 코딩을 하게 될 경우 4줄이면 된다\n",
    "~~~ python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train) # X는 데이터, y는 답에 해당한다\n",
    "pred = model.predict(X_test)\n",
    "~~~\n",
    "<br>\n",
    "\n",
    "## Logistic Regression\n",
    "- 이진 분류 규칙은 0과 1의 두 클래스를 갖는 것으로, **일반 선형 회귀 모델을 이진분류에 사용하기 어려움**\n",
    "- 로지스틱 함수를 사용하여 **로지스틱 회귀 곡선으로 변환하여 이진 분류**할 수 있음\n",
    "~~~ python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train) # X는 데이터, y는 답에 해당한다\n",
    "pred = model.predict(X_test)\n",
    "~~~\n",
    "<br>\n",
    "\n",
    "## K-Nearest Neighbor\n",
    "- 새로운 데이터가 주어졌을 때 기존 데이터 가운데 **가장 가까운 k개의 이웃의 정보**로 새로운 데이터를 예측하는 방법론\n",
    "- 알고지름이 간단하며 큰 데이터셋과 고차원 데이터에 적합하지 않음\n",
    "~~~ python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3) # 가장 가까운 3개의(k=3) 이웃정보 (k를 하이퍼 파라미터라고 한다)\n",
    "knn.fit(X_train,y_train) # X는 데이터, y는 답에 해당한다\n",
    "pred = knn.predict(X_test)\n",
    "~~~\n",
    "<br>\n",
    "\n",
    "## Decision Tree\n",
    "- 분류와 회귀 작업이 가능한 다재다능한 머신러닝 알고리즘\n",
    "- 복잡한 데이터셋도 학습할 수 있으며 강력한 머신러닝 알고리즘인 **랜덤 포레스트(앙상블)** 의 기본 구성 요소\n",
    "~~~ python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=2)\n",
    "model.fit(X_train,y_train) # X는 데이터, y는 답에 해당한다\n",
    "pred = model.predict(X_test)\n",
    "~~~\n",
    "<br>\n",
    "\n",
    "## Random Forest\n",
    "- **일련의 예측기(분류, 회귀모델)로부터 예측을 수집하면** 가장 좋은 모델 하나보다 **더 좋은 예측**을 얻을 수 있음\n",
    "- 일련의 예측기 : 앙상블\n",
    "- 결정 트리의 앙상블 : 랜덤 포레스트\n",
    "- 훈련 세트로부터 무작위로 각기 **다른 서브셋**을 만들어 일련의 결정 트리 분류기를 훈련시킬 수 있음\n",
    "~~~ python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimatiors = 50)\n",
    "model.fit(X_train,y_train) # X는 데이터, y는 답에 해당한다\n",
    "pred = model.predict(X_test)\n",
    "~~~\n",
    "<br>\n",
    "\n",
    "# Ensemble 기법\n",
    "- 여러 개의 분류기를 생성하고, 그 **예측을 결합**함으로써 **보다 정확한 예측**을 도출하는 기법\n",
    "- 약한 모델 여러 개를 조합하여 더 정확한 예측 방식\n",
    "- 적절한 **Hyperparameter 튜닝**이 중요\n",
    "\n",
    "## 1) Boosting\n",
    "- 이전 학습에 대하여 잘못 예측된데이터에 가중치를 부여해 오차를 보완해 나가는 방식\n",
    "- **순차적인 학습**을 하여 weight를 부여해서 오차를 보완하므로 **학습 시간이 길 수 있음**\n",
    "- 대표 모델(외우기) : XGBoost, LightGBM\n",
    "\n",
    "## 2) Stacking\n",
    "- 여러 개 모델이 예측한 결과 데이터를 기반으로 final_estimator 모델로 종합하여 예측 수행\n",
    "- 성능은 향상될 수 있으나 과대적합을 유발할 수 있음\n",
    "\n",
    "## 3) Weighted Blending\n",
    "- 각 모델의 예측값에 대하여 weight를 곱하여 최종 output 계산\n",
    "- 가중치의 합은 **1.0**이 되도록 함\n",
    "<br>\n",
    "\n",
    "## Ensemble 대표 모델 1. XGBoost\n",
    "~~~ python\n",
    "!pip install xgboost #주피터 노트북에서 리눅스 명령어로 XGBoost 설치하는 명령어\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(n_estimators=50)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "~~~\n",
    "\n",
    "<br>\n",
    "\n",
    "## Ensemble 대표 모델 2. LightGBM\n",
    "~~~ python\n",
    "!pip install lightgbm #주피터 노트북에서 리눅스 명령어로 XGBoost 설치하는 명령어\n",
    "from xgboost import LGBMClassifier\n",
    "model = LGBMlassifier(n_estimators=50)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624882da-d92c-4d0d-877d-b3ec551ab0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
